// :hp-image: /covers/cover.png

= Proxmox VE: Virtualization for Work and Play (Getting Started)
:hp-alt-title: Server Virtualization Management Part2
:hp-tags: Blog, Open_Source, Technology
:icons: image
:linkattrs:
:published_at: 2017-04-25
:toc: macro
:toclevels: 3

In the link:/2017/04/23/Server-Virtualization-Management[previous post] we learned about ProxMox and outlined our plan to build a powerful "bang for the buck" home server for games _and_ other system-intensive pursuits. Now we begin.

toc::[]

== Installation

https://forum.level1techs.com/t/proxmox-zfs-with-ssd-caching-setup-guide/97663
https://pve.proxmox.com/wiki/Quick_installation

Disconnect all the drives except the boot drive
KVM

== Post-Install

=== GPU Passthrough

GPU Passthrough allows our VM to access GPU hardware for games, graphics and heavy computation (i.e. deep learning). We must enable link:https://en.wikipedia.org/wiki/Input%E2%80%93output_memory_management_unit[IOMMU ("Input–output memory management unit")^] drivers, which allocate device-visible virtual addresses to the actual physical addresses. IOMMU enables our VM to communicate with our GPU using the virtual addresses as-if it were directly communicating to the GPU.

link:https://www.kernel.org/doc/Documentation/vfio.txt[VFIO ("Virtual Function I/O")^] modules are part of an IOMMU device-agnostic framework for exposing direct device access to userspace, in a secure, IOMMU protected environment.  In other words, they provide access to non-privileged, low-overhead userspace drivers.

==== Include the VFIO Modules:

. Type: `vi /etc/modules`
. Enter edit mode: `i`
.. Add: `vfio`
.. Add: `vfio_iommu_type1`
.. Add: `vfio_pci`
.. Add: `vfio_virqfd`
. Exit edit mode: <esc>
. Save and exit: `wq <enter>`

==== Configure the VFIO Modules

Identify the GPU to passthrough:

. Identify all GPUs: `lspci -nn | grep VGA`
. Capture the GPU slot IDs (first pair of numbers separated by a colon):
.. My GPU Slot ID for passthrough is: `28:00`
.. My GPU Slot ID for the host is: `21:00`
. Capture the vendor IDs for the passthrough GPU Slot ID: `lspci -nns 00:02 | cut -d "]" -f 2 | cut -d "[" -f 2`
.. The vendor ID for my GPU VGA device: `10de:1b80`
.. The vendor ID for my GPU Audio device: `10de:10f0`

To enable KVM passthrough, add the following module options (including the comma separated vendor IDs identified in the prior step). This loads the vfio-pci kernel module, which maps memory regions from the PCI bus to the VM, and activates support for IOMMU groups.

. Type: `vi /etc/modprobe.d/kvm.conf`
. Enter edit mode: `i`
.. `options vfio_iommu_type1 allow_unsafe_interrupts=1`
.. `options vfio-pci         ids=*10de:1b80,10de:10f0*`
.. `options vfio-pci         disable_vga=1`
.. `options kvm-amd          npt=0`
.. `options kvm              ignore_msrs=1`
. Exit edit mode: <esc>
. Save and exit: `wq <enter>`

// Nested Page Table... Linux guests with Q35 and OVMF may work with npt on or off. However a Linux guest with i440fx only works with npt disabled. 

==== Update Boot Settings

Configure IOMMU and VFIO to load first so that framebuffer drivers don’t grab the GPU first while booting. After these changes, we need to commit them to grub and generate a new initrd image.

. Type: `vi /etc/default/grub`
. Enter edit mode: `i`
.. Change: `GRUB_CMDLINE_LINUX_DEFAULT="quiet"`
.. To: `GRUB_CMDLINE_LINUX_DEFAULT="quiet amd_iommu=on kvm_amd.avic=1 rd.driver.pre=vfio-pci video=efifb:off"`
. Exit edit mode: <esc>
. Save and exit: `wq <enter>`
. Run: `update-grub`
. Run: `update-initramfs -u`

//iommu=pt
// AMD SVM Advance Virtual Interrupt Controller (AVIC) support virtualizes local APIC registers of each vCPU via the virtual APIC (vAPIC) backing page. This allows guest access to certain APIC registers without the need to emulate the hardware behavior and should speed up workloads which generate large amount of interrupts.
// -machine q35,accel=kvm,mem-merge=off
// -cpu host,kvm=off,hv_vendor_id=vgaptrocks,hv_relaxed,hv_spinlocks=0x1fff,hv_vapic,hv_time

Reboot. To check that the driver loaded correctly, run: "`lspci --nnks 28:00`". If everything went well, we should see: "`Kernel driver in use: *vfio-pci*`".

`ll /sys/bus/pci/drivers/vfio-pci/* | grep 28:00`

// https://pve.proxmox.com/wiki/Pci_passthrough 
// lspci -nn | grep `lspci | grep VGA | cut -d "." -f1` 

=== Emulation Settings

Configure emulation settings for sound and passthrough. My Windows 7 virtual machine has an ID equal to 101; update the next command with your Windows VM ID number.

. Type: `vi /etc/pve/qemu-server/*101*.conf`
.. Enter edit mode: `i`
.. Update the following: `cpu: Opteron_G5,hidden=1`
.. Add the following:
... Audio passthrough: `args: -device intel-hda,id=sound5,bus=pcie.0,addr=0x18 -device hda-micro,id=sound5-codec0,bus=sound5.0,cad=0 -device hda-duplex,id=sound5-codec1,bus=sound5.0,cad=1`
... GPU for passthrough: `hostpci0: 28:00,pcie=1`
... Emulation for PCI-E passthrough: `machine: q35`
.. Exit edit mode: <esc>
. Save and exit: `wq <enter>`

=== Security

https://www.kiloroot.com/secure-proxmox-install-sudo-firewall-with-ipv6-and-more-how-to-configure-from-start-to-finish/


=== Optimization

Configure Ryzen to appear to have 2 sockets, 4 cores, and 2 threads

Remove Proxmox License Nag: sed -i.bak "s/data.status !== 'Active'/false/g" /usr/share/pve-manager/ext6/pvemanagerlib.js

For good performance, we need to configure SPICE (Simple Protocol for Independent Computing Environments). The SPICE packages include drivers (QXL and virtio) that enhance virtualization performance:

* SPICE Client (virt-viewer) for Linux, Windows, and Mac systems
* SPICE Guest Tools for the virtual machines

https://pve.proxmox.com/wiki/Paravirtualized_Block_Drivers_for_Windows

https://pve.proxmox.com/wiki/Windows_7_guest_best_practices

https://pve.proxmox.com/wiki/SPICE

https://www.spice-space.org/download.html


