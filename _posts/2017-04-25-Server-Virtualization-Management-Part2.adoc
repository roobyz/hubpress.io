// :hp-image: /covers/cover.png

= PVE: Virtualization for Work and Play (Part 2)
:hp-alt-title: Server Virtualization Management Part2
:hp-tags: Blog, Open_Source, Technology
:icons: image
:linkattrs:
:published_at: 2017-04-25
:toc: macro
:toclevels: 3

== Getting Started...

In the link:/2017/04/23/Server-Virtualization-Management[previous post], we learned about ProxMox Virtual Environment (PVE) and outlined the plan to build a powerful "bang for the buck" home server for games _and_ other system-intensive pursuits. Before proceeding, you should feel a little comfortable with the link:http://linuxcommand.org/lc3_learning_the_shell.php[Linux CLI (command line interface)^]. Now let's begin.

toc::[]

== Installation

The link:https://pve.proxmox.com/wiki/Quick_installation[PVE Quick Installation^] guide does a wonderful job of highlighting the key installation points and showing how simple the process really is. If you follow the defaults settings, PVE will install to _local disks_ and should take about 10 minutes or less. If you have a large single drive that you want to use, then you can skip to the post-installation section below since that guide is all that you would need.

[cols="1, 8a, 1"]
|===
|
|PVE Installation Process
image:Server-Virtualization-Management/pve-installation.gif[pve-install]
|
|===

=== PVE Drive Options

For our setup, we will disconnect all the drives except for our boot drive, and follow most of the default installation options with one exception. Since we only want to use half of the boot drive (512Gb NVMe) our _hard disk options_ are as follows: 

* *`ext4`* _filesystem_: Standard Linux filesystem is the safe bet.
* *`176.0`* _hdsize_: Shooting for half of our 512Gb. The following values should add up to 256Gb. Equal to 256Gb-80Gb (minfree).
* *`64.0`* _swapsize_: Linux swap file size (equal to our ram size). Be sure to set vm.swappiness to a low value if you have your swap file on an SSD! It'll increase RAM usage a bit, but will be easier on our SSD.
* *`96.0`* _maxroot_: / root file partition 
* *`80.0`* _minfree_: This should equal our ZFS log (16GB) plus our ZFS cache (64GB).
* *`16.0`* _maxve_: This is the pve-data partition.

When you get to the _Installation Successful_ step of the PVE install, click the "Reboot" button.

=== ZFS Partitions

After rebooting you can log in via the PVE Web GUI or through the command line using SSH. Logging in via ssh:
```
ssh root@10.10.1.10

#The authenticity of host '10.10.1.10 (10.10.1.10)' can't be established.
#ECDSA key fingerprint is SHA256:2ExP+SHaCo+9ZOt+sk90DPLAafdHFJTHPyeU1qtFXIg.
#Are you sure you want to continue connecting (yes/no)? 
```
Type "yes" and then enter the password set during installation. 

After logging into our new PVE installation, we want to add two additional partitions (ZFS Log (16Gb), and ZFS Cache(64Gb)). The combined storage will be 256Gb, leaving us with half of our NVMe for other options like dual-boot, additional storage, etc.

After logging in, run (update for your drive): `cfdisk /dev/sda`

Go down to the "free space" line in green and add a 16Gb partition. Move down again to the next "free space" line in green and add a 64Gb partition. Then select `write` and then select `quit` and we are done.

[cols="1, 8a, 1"]
|===
|
|Creating ZFS Log and Cache Partitions
image:Server-Virtualization-Management/pve-cfdisk-process.gif[pve-cfdisk]
|
|===

Once these two partitions are added, we can shut down PVE from the command line: 
```
shutdown -h
```

=== ZFS Setup

Once PVE has shut down, we can reconnect the remaining drives and restart our system. Before setting up our ZFS storage, we must backup any data that we want to keep.

Let's start our link:http://open-zfs.org/wiki/Performance_tuning[ZFS configuration^]. As mentioned in our previous post, we are configuring ZFS as striped-mirrored storage. Since we have a 2TB spinning disk that we want to use for backup, we will mirror it as an _automatic_ backup. 

Our drives should all be the same size, otherwise, we will lose storage capacity. Since our SSD drives are 1TB each, we need to partition our 2TB spinning disk to two 1TB partitions. Before partitioning, identify the correct drives; run `lsblk` to get the list of block devices:

[cols="1, 8a, 1"]
|===
|
|List of block devices
image:Server-Virtualization-Management/pve-lsblk.png[pve-lsblk]
|
|===

In my example, the 2TB (1.8T) drive is `/dev/sdc`. The following commands will replace the drive with a new GPT partition table and create the 2 partitions:

```
# Install parted
apt-get install parted

# remove everything on the /dev/sdc drive and 
# replace with two empty equal-sized partitions
parted /dev/sdc --script mklabel gpt \
       mkpart primary 0% 50% \
       mkpart primary 50% 100% p
```

After partitioning, we can mirror and stripe the drives. When we create the drive mirrors, ZFS creates virtual devices (vdevs). We can then connect the vdevs together into zpools. For example, we can _mirror_ two 1TB drives and we end up with a 1TB vdev that will automatically replicate our data across both drives. Then we combine the two 1TB mirrored vdevs and end up with 2TB of storage.

Since the zpool read/write transactions are balanced across the two vdevs, we can actually get an increase in drive performance with the transactions happening in parallel across two physical drives. We can also compress the read-write transactions on the zpool. Because our CPU can compress-decompress data much faster than the drives can read-write data, our drive performance can improve even more, because of the smaller size of the read-write transactions on the zpool.

.Creating our ZFS Storage Pool
[cols="5a, 5a"]
|===
|* zpool create -o ashift=12 tank \
* mirror /dev/sda /dev/sdc1 \
* mirror /dev/sdb /dev/sdc2 \
* log   /dev/nvm0n1p4 \
* cache /dev/nvm0n1p5

|* pool called tank with 4k sectors
* first vdev
* second vdev
* 16GB log partition
* 64GB cache partition
|===

```
zfs set compression=lz4 tank  # lz4 pool compression
zfs create tank/vm-disks      # ZFS layer to store VM images
```

Once that's done, we can run the following commands:

```
zpool list          # verify that our pool has been created
zpool status tank   # check pool status and configuration
pvesm zfsscan       # list available ZFS file systems
```

== Post-Installation

The PVE open-source license allows for testing and non-production use. If we would like to use PVE for production or we want commercial support, we can purchase a subscription, enter our key through the web interface, and skip to the "Update PVE" section.

=== Adjusting the PVE Repositories

The link:https://pve.proxmox.com/wiki/Package_Repositories[PVE Package Repositories^] can be configured depending on your usage goals. Let's include the non-commercial list of repositories. 

Run `nano /etc/apt/sources.list` and update as follows:
```
# main debian repo
deb http://ftp.us.debian.org/debian stretch main contrib

# security updates
deb http://security.debian.org stretch/updates main contrib
```
Save and exit: press CTRL+X, Y for yes, and ENTER.

Comment-out the PVE commercial repository.

Run `nano /etc/apt/sources.list.d/pve-enterprise.list` and update as follows:
```
# non-subscription repo (manual update)
deb http://download.proxmox.com/debian/pve stretch pve-no-subscription
#deb https://enterprise.proxmox.com/debian/pve stretch pve-enterprise
```
Save and exit: press CTRL+X, Y for yes, and ENTER.

=== Update PVE

Edit our _resume_ settings: run `nano /etc/initramfs-tools/conf.d/resume` and add:
```
RESUME=none
```
Save and exit: press CTRL+X, Y for yes, and ENTER.

Update the software packages, boot loader, and system image. From the PVE, command line type:

```
apt-get update && apt-get upgrade -y
update-grub
update-initramfs -u
```

=== Update the PVE Storage System:

Once we create our ZFS storage, we can go to the PVE Web GUI and add it to our setup. Being sure to use _HTTPS_, open https://machine-ip-address:8006 in a web browser. When we get the _certificate warning_ message, we should proceed anyway. This happens because the machine does not have a certificate signed by a third party. Our goal is to end up with four storage volumes:

.PVE storage volumes.
[cols="3a, 8a"]
|===
|. vm-disks
. zfs-backups
. zfs-containers
. zfs-templates

|* Stores RAW disk images more efficiently
* Stores VZDump backups of virtual machines
* Stores LXC container filesystems
* Stores ISOs and container templates

|===

Once logged in, we go to Datacenter > Storage, and:

. click *Add* > *ZFS*, then enter "*_vm-disks_*" for ID, and select _tank/vm-disks_ for pool, choose only _Disk Image_ for content, and finally tick the _Thin Provision_ checkbox and select *Add*.
. click *Add* > *ZFS*, then enter "*_zfs-containers_*" for ID, and select _tank_ for pool, and _Container_ for content, and select *Add*.
. click *Add* > *Directory*, then enter "*_zfs-backups_*" for ID, enter "_/tank_" (/our-zfs-pool) for directory, and choose only _VZDump backup files_ for content, then select *Add*. 
. click *Add* > *Directory*, then enter "*_zfs-templates_*" for ID, enter "_/tank_" (/our-zfs-pool) for directory, and choose both _container templates_ and _ISO images_ for content, then select *Add*. 

After adding our new storage options, we can disable the local storage:

. select *local-lvm*, click *Edit*, untick the _Enable_ checkbox, and click "OK".
. select *local*, click *Edit*, untick the _Enable_ checkbox, add "1" for _Max Backups_, and then click "OK".

Afterward, if we select the arrow next to pve in the _Server View_, we will only see only four enabled storage options.

[cols="1, 8a, 1"]
|===
|
|PVE Storage Volume Setup
image:Server-Virtualization-Management/pve-zfs-setup.gif[pve-zfs-setup]
|
|===

We made it! With only one storage volume for each type of content, there's no way to accidentally misplace something. Creating containers and VMs should function as expected. Our machine is ready to go. 

This is part 2 of our multipart tutorial. When complete, our next installment will cover some opportunities for _System Optimization_.

* Part 3: System Optimization

